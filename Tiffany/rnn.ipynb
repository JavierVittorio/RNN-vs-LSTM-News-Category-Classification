{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c474ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c4be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 headline   category\n",
      "0       Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS\n",
      "1       American Airlines Flyer Charged, Banned For Li...  U.S. NEWS\n",
      "2       23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY\n",
      "3       The Funniest Tweets From Parents This Week (Se...  PARENTING\n",
      "4       Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS\n",
      "...                                                   ...        ...\n",
      "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...       TECH\n",
      "209523  Maria Sharapova Stunned By Victoria Azarenka I...     SPORTS\n",
      "209524  Giants Over Patriots, Jets Over Colts Among  M...     SPORTS\n",
      "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...     SPORTS\n",
      "209526  Dwight Howard Rips Teammates After Magic Loss ...     SPORTS\n",
      "\n",
      "[209527 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "path = r\"C:\\Users\\Tiffany\\Downloads\\KULIAH\\SEMESTER V\\DEEP LEARNING\\EKSPLORASI\\LSTM VS RNN\\RNN-vs-LSTM-News-Category-Classification\\News_Category_Dataset_v3.json\"\n",
    "\n",
    "df = pd.read_json(path, lines=True)\n",
    "df.head()\n",
    "\n",
    "print(df[['headline', 'category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d7c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tiffany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tiffany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>cleaned_headline</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters</td>\n",
       "      <td>million american roll sleeve omicrontargeted covid booster</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video</td>\n",
       "      <td>american airline flyer charged banned life punching flight attendant video</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)</td>\n",
       "      <td>funniest tweet cat dog week sept</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Sept. 17-23)</td>\n",
       "      <td>funniest tweet parent week sept</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Loses Lawsuit Against Ex-Employer</td>\n",
       "      <td>woman called cop black birdwatcher loses lawsuit exemployer</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    headline  \\\n",
       "0               Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters   \n",
       "1  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video   \n",
       "2                      23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)   \n",
       "3                                   The Funniest Tweets From Parents This Week (Sept. 17-23)   \n",
       "4              Woman Who Called Cops On Black Bird-Watcher Loses Lawsuit Against Ex-Employer   \n",
       "\n",
       "                                                             cleaned_headline  \\\n",
       "0                  million american roll sleeve omicrontargeted covid booster   \n",
       "1  american airline flyer charged banned life punching flight attendant video   \n",
       "2                                            funniest tweet cat dog week sept   \n",
       "3                                             funniest tweet parent week sept   \n",
       "4                 woman called cop black birdwatcher loses lawsuit exemployer   \n",
       "\n",
       "    category  \n",
       "0  U.S. NEWS  \n",
       "1  U.S. NEWS  \n",
       "2     COMEDY  \n",
       "3  PARENTING  \n",
       "4  U.S. NEWS  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing Data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = text.lower()                            # 1. lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)            # 2. remove url\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)           # 3. remove punctuation & numbers\n",
    "    tokens = text.split()                          # 4. tokenization\n",
    "    tokens = [w for w in tokens if w not in stop_words]  # 5. remove stopwords\n",
    "    tokens = [lemm.lemmatize(w) for w in tokens]   # 6. lemmatization\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def csv_preprocess(df):\n",
    "    # Hapus data duplikasi pada kolom headline\n",
    "    df = df.drop_duplicates(subset='headline', keep='first')\n",
    "        \n",
    "    # Remove barus dengan null values\n",
    "    df = df.dropna()\n",
    "        \n",
    "    # Reset index setelah penghapusan baris\n",
    "    df = df.reset_index(drop=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df['cleaned_headline'] = df['headline'].apply(text_preprocess)\n",
    "df = csv_preprocess(df)\n",
    "df[['headline', 'cleaned_headline', 'category']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c27d26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "0    17870\n",
      "1    17319\n",
      "2     9873\n",
      "3     9323\n",
      "4     8723\n",
      "5     6330\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "categories_to_keep = ['WELLNESS', 'ENTERTAINMENT', 'TRAVEL', 'STYLE & BEAUTY', 'PARENTING', 'FOOD & DRINK']\n",
    "\n",
    "# Create a mapping dictionary for encoding\n",
    "category_mapping = {\n",
    "    'WELLNESS': 0,\n",
    "    'ENTERTAINMENT': 1,\n",
    "    'TRAVEL': 2,\n",
    "    'STYLE & BEAUTY': 3,\n",
    "    'PARENTING': 4,\n",
    "    'FOOD & DRINK': 5\n",
    "}\n",
    "\n",
    "# Fix the filtering - use .keys() method\n",
    "df = df[df['category'].isin(category_mapping.keys())]\n",
    "\n",
    "# Apply the encoding to the category column\n",
    "df['category'] = df['category'].map(category_mapping)\n",
    "\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42ad5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum samples per category: 6330\n",
      "\n",
      "Balanced dataset shape: (37980, 7)\n",
      "\n",
      "Category distribution after balancing:\n",
      "category\n",
      "0    6330\n",
      "1    6330\n",
      "2    6330\n",
      "3    6330\n",
      "4    6330\n",
      "5    6330\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiffany\\AppData\\Local\\Temp\\ipykernel_2392\\4130417362.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df.groupby('category', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Balance the dataset by limiting each category to the minimum count\n",
    "min_samples = df['category'].value_counts().min()\n",
    "print(f\"Minimum samples per category: {min_samples}\")\n",
    "\n",
    "# Sample the same number of rows from each category\n",
    "df_balanced = df.groupby('category', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=min_samples, random_state=42)\n",
    ")\n",
    "\n",
    "# Reset index and shuffle\n",
    "df_balanced = df_balanced.reset_index(drop=True)\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Update df with balanced dataset\n",
    "df = df_balanced\n",
    "\n",
    "print(f\"\\nBalanced dataset shape: {df.shape}\")\n",
    "print(f\"\\nCategory distribution after balancing:\")\n",
    "print(df['category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be54646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah teks menjadi vector (Tokenizing)\n",
    "import tensorflow as tf\n",
    "\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
    "Tokenizer = tf.keras.preprocessing.text.Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['cleaned_headline'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_headline'])\n",
    "\n",
    "MAX_LEN = 12\n",
    "\n",
    "X_padded = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_padded, df['category'], test_size=0.2, random_state=42, stratify=df['category']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7ef9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arsitektur model LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=0.0001)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 128, input_length=15),\n",
    "    tf.keras.layers.SimpleRNN(128, dropout=0.25, return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(64, dropout=0.25, return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(32),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='softmax'),  # ✅ FIXED: Changed sigmoid to softmax\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccdb6f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6039 - loss: 1.0794 - val_accuracy: 0.7504 - val_loss: 0.7574 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.8132 - loss: 0.5894 - val_accuracy: 0.7765 - val_loss: 0.6938 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8664 - loss: 0.4277 - val_accuracy: 0.7884 - val_loss: 0.6710 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.8943 - loss: 0.3410 - val_accuracy: 0.7848 - val_loss: 0.7426 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.9121 - loss: 0.2844 - val_accuracy: 0.7788 - val_loss: 0.7898 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.9485 - loss: 0.1714 - val_accuracy: 0.7933 - val_loss: 0.7861 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9097385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7884 - loss: 0.6710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6709970831871033, 0.7884413003921509]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b42bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Predictions shape: (7596, 6)\n",
      "\n",
      "First 5 predictions:\n",
      "[[0.00265196 0.01181421 0.9730752  0.00713341 0.00217453 0.0031506 ]\n",
      " [0.02821628 0.00172325 0.10447009 0.00482159 0.01237246 0.84839636]\n",
      " [0.00293026 0.00543158 0.9793553  0.00722898 0.00222596 0.00282796]\n",
      " [0.9286876  0.00131662 0.01339749 0.00424482 0.04543672 0.00691683]\n",
      " [0.6287197  0.03828933 0.13926685 0.03103549 0.11592986 0.04675887]]\n",
      "\n",
      "First sample probabilities (sum=1.0000):\n",
      "  Class 0: 0.0027 (0.27%)\n",
      "  Class 1: 0.0118 (1.18%)\n",
      "  Class 2: 0.9731 (97.31%)\n",
      "  Class 3: 0.0071 (0.71%)\n",
      "  Class 4: 0.0022 (0.22%)\n",
      "  Class 5: 0.0032 (0.32%)\n",
      "\n",
      "First 10 predicted classes: [2 5 2 0 0 2 0 4 5 0]\n",
      "First 10 actual classes: [2 5 2 0 0 2 0 4 5 0]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions (probabilities for all 6 classes)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Show shape: should be (num_samples, 6)\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"\\nFirst 5 predictions:\")\n",
    "print(predictions[:5])\n",
    "\n",
    "# For each prediction, you get 6 probabilities that sum to 1.0\n",
    "print(f\"\\nFirst sample probabilities (sum={predictions[0].sum():.4f}):\")\n",
    "for i, prob in enumerate(predictions[0]):\n",
    "    print(f\"  Class {i}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "# Get the predicted class (highest probability)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(f\"\\nFirst 10 predicted classes: {predicted_classes[:10]}\")\n",
    "print(f\"First 10 actual classes: {y_test.values[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
